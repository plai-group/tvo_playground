import torch
import torch.nn as nn


class MLP(nn.Module):
    def __init__(self, dims, non_linearity):
        """
        Args:
            dims: list of ints
            non_linearity: differentiable function

        Returns: nn.Module which represents an MLP with architecture

            x -> Linear(dims[0], dims[1]) -> non_linearity ->
            ...
            Linear(dims[-3], dims[-2]) -> non_linearity ->
            Linear(dims[-2], dims[-1]) -> y"""

        super(MLP, self).__init__()
        self.dims = dims
        self.non_linearity = non_linearity
        self.linear_modules = nn.ModuleList()
        for in_dim, out_dim in zip(dims[:-1], dims[1:]):
            self.linear_modules.append(nn.Linear(in_dim, out_dim))

    def forward(self, x):
        temp = x
        for linear_module in self.linear_modules[:-1]:
            temp = self.non_linearity(linear_module(temp))
        return self.linear_modules[-1](temp)


class MLPTwoProng(nn.Module):
    def __init__(self, dims, non_linearity):
        """
        Args:
            dims: list of ints
            non_linearity: differentiable function

        Returns: nn.Module which represents an MLP with architecture

            x -> Linear(dims[0], dims[1]) -> non_linearity ->
            ...
            Linear(dims[-3], dims[-2]) ->
            non_linearity -> Linear(dims[-2], dims[-1]) -> mu
                          -> Linear(dims[-2], dims[-1]) -> exp -> std

        """

        super(MLPTwoProng, self).__init__()
        self.dims = dims
        self.non_linearity = non_linearity
        self.linear_modules = nn.ModuleList()
        for in_dim, out_dim in zip(dims[:-1], dims[1:]):
            self.linear_modules.append(nn.Linear(in_dim, out_dim))
        self.logsigma = nn.Linear(in_dim, out_dim)

    def forward(self, x):
        temp = x
        for linear_module in self.linear_modules[:-1]:
            temp = self.non_linearity(linear_module(temp))
        mu = self.linear_modules[-1](temp)
        sig = torch.exp(self.logsigma(temp))
        return mu, sig


class ControlVariate(nn.Module):
    def __init__(self, num_mixtures, device=torch.device('cpu')):
        super(ControlVariate, self).__init__()
        self.num_mixtures = num_mixtures
        self.mlp = nn.Sequential(
            nn.Linear(num_mixtures, 16),
            nn.Tanh(),
            nn.Linear(16, 16),
            nn.Tanh(),
            nn.Linear(16, 1)
        )
        self.device = device

    def forward(self, aux, mean=True):
        """Args:
            aux: tensor of shape [batch_size, S, num_mixtures]
            obs: tensor of shape [batch_size]

        Returns: tensor of shape [batch_size]
        """
        batch_size, S, num_mixtures = aux.shape
        input = aux.view(-1, num_mixtures)
        output = self.mlp(input).squeeze(-1).view(batch_size, S)
        if mean:
            return torch.mean(output, dim=1)
        else:
            return output


def init_mlp(in_dim, out_dim, hidden_dim, num_layers, non_linearity):
    """Initializes a MultilayerPerceptron.

    Args:
        in_dim: int
        out_dim: int
        hidden_dim: int
        num_layers: int
        non_linearity: differentiable function

    Returns: a MultilayerPerceptron with the architecture

        x -> Linear(in_dim, hidden_dim) -> non_linearity ->
        ...
        Linear(hidden_dim, hidden_dim) -> non_linearity ->
        Linear(hidden_dim, out_dim) -> y

        where num_layers = 0 corresponds to

        x -> Linear(in_dim, out_dim) -> y"""
    dims = [in_dim] + [hidden_dim for _ in range(num_layers)] + [out_dim]
    return MLP(dims, nn.Tanh())


def init_two_prong_mlp(in_dim, out_dim, hidden_dim, num_layers, non_linearity=nn.Tanh()):
    """Initializes a MultilayerPerceptronNormal.

    Args:
        in_dim: int
        out_dim: int
        hidden_dim: int
        num_layers: int
        non_linearity: differentiable function

    Returns: a MultilayerPerceptron with the architecture

        x -> Linear(in_dim, hidden_dim) -> non_linearity ->
        ...
        Linear(hidden_dim, hidden_dim) -> non_linearity ->
        Linear(hidden_dim, out_dim) -> y

        where num_layers = 0 corresponds to

        x -> Linear(in_dim, out_dim) -> mu
          -> Linear(in_dim, out_dim) -> exp -> std
        """
    dims = [in_dim] + [hidden_dim for _ in range(num_layers)] + [out_dim]
    return MLPTwoProng(dims, non_linearity)
